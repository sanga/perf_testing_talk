<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

    body {
        font-family: 'Droid Serif';
        font-size-adjust: 1.1;
    }
    h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
    }
    h1 {
        color: #E76F0E;
    }

    h2,
    h3 {
        color: #A1BFCA
    }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">


layout: true

class: centre, middle

---

# Performance Testing

---

### Context

---

### Web perf testing

in a large (ish) distributed system.

---

A cloud backup service

- 16 application servers
- 32 DB shards
- 100 million files

???
Basically this talk is a summary of two years of doing perf testing and monitoring tooling in that system.
Not everything here may apply to other systems but certainly some of it will
---
## Some basic comments about performance testing
---
## Debugging perf problems is hard

Knowing **when** things slowed down can help **a lot**

Try to minimise what changed between the fast and slow revisions

Run perf tests often

???
As a battle-story. Our API servers weren't performing and we didn't know why (this was before we started perf testing in earnest). Took us about 2 months to figure out it what down to a single line! (a ref to a logger object was created once per object instantiation and those refs aren't properly cleaned up in python)
---

### load generation

### monitoring

---

## Monitoring

This is the problem you should solve first

---

You already have load generated by your users, in production

---
If you don't have decent monitoring in place you have no visibility into your applications performance
---

- Where are the system bottlenecks?
- When did things become slower?

---

First make sure you have a good enough monitoring stack in place that you can understand how your system is performing

---
Monitoring is a massive topic in itself and out of scope for this session.

???
I ended up rebuilding the monitoring stack for that system, so it's an area of interest for me
---
# load generation
---
### Model real world load

This is not trivial

---

### Solutions
---
### Create a "best effort" perf scenario yourself

- Check production logs
- Look at client logic etc

---
It's difficult to get this right
---

In a large system, even your "toy" perf testing env will be big and expensive

Note: it should be an isolated environment also
---
It'll likely eat a lot of time to maintain as you'll be dealing with a lots of environmental problems (network, hardware problems etc)
---

### Rely on production load

Instead, monitor production performance tightly in production and make rollback easy

- canaries
- dark load
- etc
---
Having a really nice deployment setup is lovely if you have it. If you don't it's a large amount of work to get there.
---
You need to think carefully in the case that you're releasing new feature or new client
---
You still have the large, annoying to maintain "perf test env"
---

### Don't even try

Don't model at all

Hit a few APIs to get decent coverage

"Performance unit tests"
---

- more relevant for large systems
- more relevant in a SOA-type set up
- doesn't tell you if a given perf regression is significant to your user
- may lead you to optimising things that don't have any meaningful net benefit to the end user
- won't catch perf problems due to the interaction of multiple components
---
- fast feedback
- way simpler to set up
- way more reliable
- way easier to maintain

???
This is important

---
# Tools
---
There are many ways to skin a cat

JMeter, Neoload, Gatling, locust, k6, blazemeter, tsung, ab, vegeta, ...

---

Use a programmer friendly tool

---
We had Neoload and migrated to locust
---

Neoload was a awful mash of hacky scripts, where java called js that shelled out to bash etc.
---
Perf scenario's are XML (etc)
- not human readable
- not practically diffable/reviewable.

---

### Coder-oriented perf testing tools
---

load generation scenarios are code

- small and plain text
- can be diffed
- can get code review
- etc
---
- simpler
- scenarios have the full power of a programming lang/unrestricted to "what's in the UI"
- most of them are open source, so you can hack them yourselves
- leverage existing libraries
---
# Locust
---
- lovely API
- cmdline and web ui
- distributed load generation
---
- clean separation between HTTP client and task running logic:
  - easy testing of REST APIs
  - or of other protocols too (SQL for example)
- nice event hook system for customization

---
Optimised for **developer happiness** at the expense of cost to run perf tests

More likely to get buy-in from devs
---

## Upsides and Downsides
---
### performance

Java/Go/etc are fast. Python is slow
---
You can scale you test env accordingly

One just needs to scale the SUT to guarantee that the SUT is the bottleneck
---
### python packaging mess

Locust requires python headers and c libs etc
---
Docker helps
---
### Manual scenario distribution
---
More of an inconvenience than a problem

- a bit of shell + scp
- bake the test scenarios in a container
- etc
---
### GIL

The GIL means that in practice you'll be running in "distributed mode" even if you're running load generation on a single machine
---
Again, more of an inconvenience than a real problem

---
## Examples
---
### Writing Perf scenarios using a client SDK

- Means that your client code can be higher level
- Means that API changes are potentially abstracted away by the SDK

Autogenerated SDK e.g. swagger based APIs can give you this

---
### Integrate internal and external perf metrics

Easily correlate problems in external performance metrics with their root cause

???
For example:

- cron job runs
- stressed the DB
- response times spike
- cron finishes
- response times normalise

---
# Demo
---
# Summary
---
- performance testing and monitoring go hand-in-hand
- test often - especially relevant for perf testing
- think carefully about the options you have in terms of perf testing and load generation
- use a tool that's make you happy
- locust made me happy
---
Thanks!
</textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js">
</script>
<script>
var slideshow = remark.create({
    ratio: '16:9',
    slideNumberFormat: function(current, total) {
        return '';
    }
});
</script>
</body>
</html>
